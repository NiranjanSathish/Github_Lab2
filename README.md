# GitHub Lab 2 - (IE-7374)

An automated machine learning pipeline for credit card default prediction featuring Support Vector Machine (SVM) classification with Platt scaling calibration. This lab demonstrates MLOps best practices including automated model training, probability calibration, comprehensive evaluation, and CI/CD implementation using GitHub Actions.

## Overview

This lab implements an end-to-end MLOps pipeline that predicts credit card defaults using real-world financial data. The system emphasizes **probability calibration** - transforming raw SVM outputs into reliable probability estimates critical for financial decision-making.


## Getting Started

### Prerequisites
- Python 3.9 or higher
- Git installed on your system
- GitHub account with Actions enabled

### Installation Steps

**1. Fork and Clone the Repository**

```bash
# Fork the repository on GitHub, then:
git clone https://github.com/your-username/Github_Lab2_MLOPS.git
cd Github_Lab2_MLOPS
```

**2. Set Up Your Development Environment**

Create and activate a virtual environment:

```bash
# Create virtual environment
python -m venv venv

# Activate on Windows
venv\Scripts\activate

# Activate on macOS/Linux
source venv/bin/activate
```

**3. Install Dependencies**

```bash
pip install -r requirements.txt
```

**4. Enable GitHub Actions**
- Go to repository **Settings** â†’ **Actions** â†’ **General**
- Under "Workflow permissions", select **"Read and write permissions"**
- Click **Save**

---

## ğŸ“‚ Project Architecture

```
Github_Lab2_MLOPS/
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â”œâ”€â”€ model_training_on_push.yml       # Training workflow (on push)
â”‚       â”œâ”€â”€ model_calibration_on_push.yml    # Calibration workflow (sequential)
â”‚       â””â”€â”€ model_training_scheduled.yml     # Daily automated pipeline
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ train_model.py                       # SVM training on UCI dataset
â”‚   â”œâ”€â”€ calibrate_model.py                   # Platt scaling calibration
â”‚   â””â”€â”€ evaluate_model.py                    # Comprehensive evaluation
â”‚
â”œâ”€â”€ models/                                   # Auto-generated by workflows
â”‚   â”œâ”€â”€ model_*_base.joblib                  # Uncalibrated SVM models
â”‚   â”œâ”€â”€ model_*_calibrated_sigmoid.joblib    # Calibrated SVM models
â”‚   â”œâ”€â”€ model_*_metadata.pickle              # Model metadata
â”‚   â””â”€â”€ calibration_report_*.pickle          # Calibration analysis
â”‚
â”œâ”€â”€ metrics/                                  # Auto-generated metrics
â”‚   â”œâ”€â”€ *_base_metrics.json                  # Base model performance
â”‚   â””â”€â”€ *_calibrated_metrics.json            # Calibrated model performance
â”‚
â”œâ”€â”€ data/                                     # Auto-generated datasets
â”‚   â”œâ”€â”€ X_train.pickle, y_train.pickle       # Training set (60%)
â”‚   â”œâ”€â”€ X_calib.pickle, y_calib.pickle       # Calibration set (20%)
â”‚   â”œâ”€â”€ X_test.pickle, y_test.pickle         # Test set (20%)
â”‚   â””â”€â”€ scaler.pickle                        # Fitted StandardScaler
â”‚
â”œâ”€â”€ app.py                                    # FastAPI deployment (optional)
â”œâ”€â”€ requirements.txt                          # Python dependencies
â”œâ”€â”€ .gitignore                               # Git exclusions
â””â”€â”€ README.md                                # This file
```

## ğŸ“Š Model Features

### Dataset: Credit Card Default (UCI Repository)

**Source**: [UCI Machine Learning Repository - ID: 350](https://archive.ics.uci.edu/dataset/350/default+of+credit+card+clients)

**Statistics**:
- 30,000 credit card clients from Taiwan
- 23 features: payment history, bill amounts, demographics
- Binary classification: Will client default next month?
- Class imbalance: ~22% default rate

**Features Include**:
1. LIMIT_BAL: Credit limit
2. SEX: Gender (1=male, 2=female)
3. EDUCATION: Education level
4. MARRIAGE: Marital status
5. AGE: Age in years
6-11. PAY_0 to PAY_6: Payment status (past 6 months)
12-17. BILL_AMT1 to BILL_AMT6: Bill amounts
18-23. PAY_AMT1 to PAY_AMT6: Payment amounts

---

**Platt scaling** (sigmoid calibration) transforms SVM outputs into reliable probabilities:
- Fits a logistic regression on the SVM's decision function
- Calibrates predictions to match actual default frequencies
- Essential for financial applications requiring accurate risk estimates
---

### Model Configuration

**Support Vector Machine (SVM)**:
```python
SVC(
    kernel='rbf',              # Radial Basis Function kernel
    C=1.0,                     # Regularization parameter
    gamma='scale',             # Kernel coefficient
    probability=True,          # Enable probability estimates
    class_weight='balanced',   # Handle class imbalance
    random_state=42
)
```

**Preprocessing**:
- StandardScaler (mean=0, std=1)
- Stratified train/calibration/test split
- Removes invalid data

---

## âš™ï¸ CI/CD with GitHub Actions

### Overview

This project implements **three automated GitHub Actions workflows** that demonstrate MLOps best practices. The workflows are triggered automatically and handle the complete ML pipeline from training to deployment.

### Workflow Architecture

```
Push to main
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Workflow 1: Training            â”‚
â”‚ model_training_on_push.yml      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“ (Triggers on completion)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Workflow 2: Calibration         â”‚
â”‚ model_calibration_on_push.yml   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Daily at midnight UTC
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Workflow 3: Scheduled Pipeline  â”‚
â”‚ model_training_scheduled.yml    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Workflow 1: Model Training (`model_training_on_push.yml`)

**Trigger**: Push to `main` branch

**Configuration**:
```yaml
on:
  push:
    branches:
      - main

jobs:
  train:
    runs-on: ubuntu-latest
```

**Pipeline Steps**:

| Step | Action | Purpose |
|------|--------|---------|
| 1 | Checkout code | `actions/checkout@v2` - Get latest code |
| 2 | Setup Python | `actions/setup-python@v2` - Python 3.9 environment |
| 3 | Install dependencies | `pip install -r requirements.txt` |
| 4 | Generate timestamp | Create unique model version ID |
| 5 | Set permissions | Make Python scripts executable |
| 6 | **Train SVM model** | Execute `train_model.py` with UCI dataset |
| 7 | Move model to models/ | Organize artifacts before evaluation |
| 8 | **Evaluate base model** | Execute `evaluate_model.py` for metrics |
| 9 | **Commit artifacts** | Push models, data, and metrics to repo |

**Artifacts Generated**:
- âœ… `models/model_[timestamp]_base.joblib` - Trained SVM
- âœ… `models/model_[timestamp]_metadata.pickle` - Model info
- âœ… `data/X_train.pickle`, `y_train.pickle` - Training data
- âœ… `data/X_calib.pickle`, `y_calib.pickle` - Calibration data
- âœ… `data/X_test.pickle`, `y_test.pickle` - Test data
- âœ… `data/scaler.pickle` - Fitted StandardScaler
- âœ… `metrics/[timestamp]_base_metrics.json` - Performance metrics

**Key Feature**: This workflow uses **timestamp-based versioning** for all artifacts, enabling model tracking and comparison.

---

### Workflow 2: Model Calibration (`model_calibration_on_push.yml`)

**Trigger**: Automatically after training workflow completes

**Configuration** (Sequential Execution):
```yaml
on:
  workflow_run:
    workflows: ["Model Training on Push to Main"]
    types:
      - completed
    branches:
      - main

jobs:
  calibrate:
    runs-on: ubuntu-latest
    if: ${{ github.event.workflow_run.conclusion == 'success' }}
```

**Why This Matters**: The `workflow_run` trigger ensures calibration **only runs after training succeeds**, preventing errors from missing models. This is a critical MLOps pattern.

**Pipeline Steps**:

| Step | Action | Purpose |
|------|--------|---------|
| 1 | Checkout code | Get latest repository state |
| 2 | Setup Python | Python 3.9 environment |
| 3 | Install dependencies | Required packages |
| 4 | **Pull latest changes** | Get models from training workflow |
| 5 | **Find latest model** | Auto-detect most recent trained model |
| 6 | Set permissions | Make scripts executable |
| 7 | **Calibrate model** | Apply Platt scaling with `calibrate_model.py` |
| 8 | Move calibrated model | Organize before evaluation |
| 9 | **Evaluate calibrated** | Measure calibration improvements |
| 10 | **Commit calibrated artifacts** | Push calibrated model and reports |

**Artifacts Generated**:
- âœ… `models/model_[timestamp]_calibrated_sigmoid.joblib` - Calibrated SVM
- âœ… `models/calibration_report_[timestamp].pickle` - Calibration analysis
- âœ… `metrics/[timestamp]_calibrated_metrics.json` - Calibration metrics

**Key Feature**: Automatic model discovery using `ls -t models/model_*_base.joblib | head -n1` ensures the workflow always calibrates the latest model.

---

### Workflow 3: Scheduled Training (`model_training_scheduled.yml`)

**Trigger**: Daily at midnight UTC + manual dispatch

**Configuration**:
```yaml
on:
  schedule:
    - cron: '0 0 * * *'  # Daily at midnight UTC
  workflow_dispatch:      # Manual trigger button in GitHub UI
```

**Purpose**: Automated daily retraining for production model updates. Combines both training and calibration in a single workflow.

**Pipeline Steps**:

| Step | Action |
|------|--------|
| 1-5 | Environment setup (same as Workflow 1) |
| 6 | Train base SVM model |
| 7 | Move base model to models/ |
| 8 | Evaluate base model |
| 9 | **Calibrate model** (same timestamp) |
| 10 | Move calibrated model |
| 11 | Evaluate calibrated model |
| 12 | **Commit all artifacts** (single commit) |

**Key Feature**: The `workflow_dispatch` trigger adds a **"Run workflow" button** in GitHub Actions UI for manual execution without pushing code.

---

### Workflow Execution Flow

**After you push to main**:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. GitHub detects push to main                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Training Workflow Starts                             â”‚
â”‚    - Duration: ~5-7 minutes                             â”‚
â”‚    - Status: Yellow circle (running)                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Training Completes Successfully                      â”‚
â”‚    - Commits models/data/metrics (1st commit)           â”‚
â”‚    - Status: Green checkmark                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Calibration Workflow Auto-Triggers                   â”‚
â”‚    - Waits for training artifacts                       â”‚
â”‚    - Duration: ~2-3 minutes                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. Calibration Completes Successfully                   â”‚
â”‚    - Commits calibrated model (2nd commit)              â”‚
â”‚    - Status: Green checkmark                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. Both Models Ready for Use                            â”‚
â”‚    - Base model for comparison                          â”‚
â”‚    - Calibrated model for production                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### Viewing Workflow Results

**Step 1: Navigate to Actions Tab**
- Go to your repository on GitHub
- Click the **"Actions"** tab at the top

**Step 2: View Workflow Runs**
You'll see three workflows:
- ğŸŸ¢ Model Training on Push to Main
- ğŸŸ¢ Model Calibration on Push to Main  
- ğŸŸ¡ Scheduled Model Training and Calibration

**Step 3: Inspect a Run**
Click any workflow run to see:
- âœ… Status of each step (green checkmark or red X)
- ğŸ“‹ Detailed logs for debugging
- â±ï¸ Execution time for each step
- ğŸ“ Commit history showing artifacts

**Step 4: Check Artifacts in Repository**
After workflows complete, navigate to:
- `models/` - See your trained and calibrated models
- `metrics/` - View JSON files with performance metrics
- `data/` - Inspect saved datasets

## ğŸ“ˆ Evaluation Metrics

### Classification Metrics

| Metric | Description | Interpretation |
|--------|-------------|----------------|
| **Accuracy** | Overall correctness | % of correct predictions |
| **Precision** | Positive predictive value | Of predicted defaults, how many were correct? |
| **Recall** | Sensitivity/True positive rate | Of actual defaults, how many did we catch? |
| **F1-Score** | Harmonic mean | Balance between precision and recall |
| **ROC-AUC** | Discrimination ability | Model's ability to distinguish classes |

### Calibration Metrics (Lower is Better)

| Metric | Purpose | Good Value | Impact |
|--------|---------|------------|--------|
| **Brier Score** | Probability accuracy | <0.15 | Lower = better calibrated predictions |
| **Expected Calibration Error (ECE)** | Calibration quality | <0.05 | Measures if probabilities match reality |
| **Log Loss** | Confidence penalty | <0.50 | Penalizes confident wrong predictions |
| **Calibration Gap** | Alignment measure | <0.03 | \|Mean prediction - Actual rate\| |

### Overconfidence Analysis

Tracks percentage of extreme predictions:
- **Extreme Low**: Predictions <0.1 (overconfident "no default")
- **Extreme High**: Predictions >0.9 (overconfident "default")
- **Total Extreme**: Sum of both (should decrease after calibration)

---
## FastAPI

### Running the API Locally

**1. Install FastAPI**:
```bash
pip install fastapi uvicorn
```

**2. Start the Server**:
```bash
python app.py
```

**3. Access the API**:
- API Root: http://localhost:8000
- Interactive Docs: http://localhost:8000/docs  â­ **Try it here!**
- Alternative Docs: http://localhost:8000/redoc

I had done this just to try the classification of the models, hence not deployed.

### API Endpoints

| Endpoint | Method | Purpose |
|----------|--------|---------|
| `/` | GET | API information |
| `/health` | GET | Health check |
| `/model_info` | GET | Model details |
| `/predict` | POST | Single customer prediction |
| `/batch_predict` | POST | Multiple predictions |
| `/calibration_report` | GET | Calibration improvements |

### Example Usage

**Single Prediction**:
```bash
curl -X POST http://localhost:8000/predict \
  -H "Content-Type: application/json" \
  -d '{
    "features": [20000, 2, 2, 1, 24, 26, 0, 0, 0, 0, 0, 
                 689, 0, 0, 0, 689, 0, 0, 0, 0, 0, 0, 0]
  }'
```

**Response**:
```json
{
  "default_probability": 0.234,
  "default_probability_pct": "23.40%",
  "prediction": 0,
  "prediction_label": "No Default",
  "risk_level": "MODERATE RISK",
  "recommendation": "Approve with monitoring",
  "suggested_interest_rate": "Prime rate + 5%"
}
```

## ğŸŒŸ Key Differentiators

### What Makes This Implementation Stand Out

| Aspect | Base Lab | This Implementation |
|--------|----------|---------------------|
| Dataset | Synthetic | **Real UCI credit card data** |
| Model | Random Forest | **SVM requiring calibration** |
| Calibration | Named only | **Actual Platt scaling** |
| Metrics | F1 only | **8+ including Brier, ECE** |
| Data Splits | Train/Test | **Train/Calib/Test (60/20/20)** |
| Workflows | Combined | **Separate training + calibration** |
| Deployment | None | **FastAPI with auto docs** |

---

## ğŸ”§ Troubleshooting

**Workflows not triggering?**
- Verify `.github/workflows/` folder structure exists
- Ensure YAML files are properly formatted
- Check that workflow permissions are enabled in Settings

**Model files not found?**
- Ensure training workflow completed successfully
- Check that files were committed (look in `models/` directory)
- Verify timestamp matches between training and calibration

**API not loading model?**
- Ensure you have run training workflow at least once
- Check that `models/` and `data/` directories exist and contain files
- Verify scaler.pickle exists in `data/` directory

**Local tests failing?**
- Activate virtual environment first
- Install all dependencies: `pip install -r requirements.txt`
- Check Python version: `python --version` (should be 3.9+)

---

## ğŸ“ License

This lab is developed as part of MLOps coursework (IE-7374) and is available for educational purposes.
---

**Author**: [Niranjan Sathish]  
**Course**:MLOPS (IE-7374)  
**Date**: October 2025  

**Key Takeaway**: GitHub Actions enables automated, reproducible ML pipelines where workflows can orchestrate complex dependencies, automatically version models, and maintain complete audit trails - essential capabilities for production MLOps systems.